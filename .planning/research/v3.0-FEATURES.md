# apex-backtest v3.0 Feature Research

**Date:** 2026-02-22
**Scope:** Four research areas for the v3.0 milestone
**Engine baseline:** v2.0 (427 tests, 90% coverage, SMC/OB/FVG/BOS shipped)

---

## Table of Contents

1. [ICT / Liquidity Concepts](#1-ict--liquidity-concepts)
2. [Regime Detection](#2-regime-detection)
3. [Advanced Risk Management](#3-advanced-risk-management)
4. [Multi-Asset Foundation](#4-multi-asset-foundation)

---

## 1. ICT / Liquidity Concepts

### 1.1 Background

ICT (Inner Circle Trader) concepts describe institutional order-flow mechanics.
They extend the existing SMC framework (Order Blocks, BOS/CHOCH, FVG) with
liquidity-focused ideas: where stops are clustered, how price sweeps them to
fill institutional orders, and which session windows offer the highest
probability. All four concepts interact tightly and must be implemented together
for maximum signal quality.

---

### 1.2 Liquidity Sweeps (Stop Hunts)

#### Concept

Retail traders place stop-loss orders just above swing highs (for shorts) or
just below swing lows (for longs). These clusters of stop orders represent
resting buy/sell liquidity that smart money targets.

A **Liquidity Sweep** occurs when:
1. Price wicks above/below a known swing high/low (the "sweep")
2. Price immediately reverses and closes back inside the prior range
3. The reversal is confirmed by a BOS or CHOCH in the opposite direction

The sweep candle's wick tip defines the "liquidity taken" level. After a
bullish sweep (below a swing low), a long opportunity exists; after a bearish
sweep (above a swing high), a short opportunity exists.

#### Algorithm

```python
@dataclass(frozen=True)
class LiquiditySweep:
    """Confirmed sweep of a swing-high or swing-low liquidity pool."""
    direction: str        # "bullish" (swept lows) or "bearish" (swept highs)
    swept_level: Decimal  # The swing high/low price that was taken
    sweep_high: Decimal   # Wick tip of the sweep candle
    sweep_low: Decimal
    sweep_bar_idx: int
    timestamp: datetime
    confirmed: bool       # True once reversal candle closes back inside range

class LiquiditySweepDetector:
    """
    Detection logic (called per bar, uses bar_buffer from BaseStrategy):

    For a BULLISH sweep (sweeping lows):
        Candidate: bar.low < last_swing_low.price
        Confirmation: bar.close > last_swing_low.price (wick through, close back above)
        OR: next bar's open > last_swing_low.price (immediate reversal candle)

    For a BEARISH sweep (sweeping highs):
        Candidate: bar.high > last_swing_high.price
        Confirmation: bar.close < last_swing_high.price

    Single-bar sweep (preferred for intraday):
        bar.low < swing_low AND bar.close > swing_low  => bullish sweep in one bar

    Multi-bar sweep (less clean, needs CHOCH confirmation):
        bar1.low < swing_low AND bar2 closes strongly above swing_low
    """
```

#### Key Formulas

```
sweep_depth = abs(bar.low - swing_low.price)          # Bullish sweep depth
sweep_depth_atr_ratio = sweep_depth / current_atr     # Filter: > 0.5 ATR minimum

# Rejection strength (body vs wick ratio)
body_size = abs(bar.close - bar.open)
lower_wick = bar.open - bar.low  (for bullish sweep candle)
rejection_ratio = lower_wick / (bar.high - bar.low)  # > 0.6 = strong rejection
```

#### Integration with Existing SMC

The `LiquiditySweepDetector` is a **companion to `SwingDetector`**. It reads
the same `SwingPoint` list already produced by `SwingDetector`. No new swing
detection is needed.

```python
# Inside SMCStrategy.calculate_signals(), after swing detection:
new_sweeps = self._sweep_detector.check_for_sweeps(
    event,                          # current MarketEvent
    self._swing_detector.swing_highs,
    self._swing_detector.swing_lows,
    self._current_atr,
    self._bar_count,
)
```

The sweep is a **prerequisite** for enhanced entry quality. Instead of
entering on any OB mitigation, only enter on OB mitigations that follow a
liquidity sweep in the same direction.

#### Pitfalls and Edge Cases

- **Duplicate sweeps:** The same swing level can be swept multiple times as
  price consolidates. Use a "cooldown" of N bars (e.g., 10) per level to
  avoid retrigger. Alternatively, mark swing points as "swept" after first
  sweep.
- **Equal highs/lows (EQH/EQL):** Multiple swing highs at the same price
  constitute a stronger liquidity pool. Detect "near-equal" highs within one
  ATR of each other and treat them as a cluster.
- **Partial sweep (wick just barely clears):** Apply a minimum sweep depth
  filter (e.g., `sweep_depth >= Decimal('0.1') * current_atr`) to filter out
  noise.
- **Sweep without reversal (trend continuation):** Not all sweeps reverse.
  A sweep that fails to close back inside the range is trend continuation, not
  a reversal opportunity. The `confirmed` flag on `LiquiditySweep` handles this.
- **Look-ahead bias:** Detection must use `bar.low/high` against historical
  swing points, never future bars. The existing `SwingDetector` already delays
  confirmation by `strength` bars, so sweep candidates are always in the past.

---

### 1.3 Inducement Patterns (False Breakouts)

#### Concept

Inducement (IDM) is a small, easily visible structure point that is sacrificed
by price to lure retail traders into a premature position before the real move.
It is the ICT explanation for "stop hunts before the real move."

Characteristics:
- Inducement is a lower-degree swing point (weaker fractal strength)
- It sits between a higher-degree structure point (BOS) and the next entry zone
- Price breaks the IDM swing, triggering stops/entries, then reverses to the
  actual destination

#### Detection Algorithm

```python
# IDM = a swing point that is BETWEEN the last BOS level and the current OB zone
# It has lower strength than the surrounding swing points

def detect_inducement(
    swing_highs: list[SwingPoint],
    swing_lows: list[SwingPoint],
    last_bos: StructureBreak,
    active_obs: list[OrderBlock],
    current_bar_idx: int,
) -> Optional[SwingPoint]:
    """
    For a BULLISH setup (last_bos.direction == 'bullish'):
        IDM = last swing low between:
            - bar_idx of last_bos
            - bar_idx of current bar
        that did NOT break below the OB zone

    The IDM low is a trap: retail sell stops cluster there.
    A sweep of IDM before price reaches the OB confirms the IDM.
    """
```

#### Integration

Inducement refines entry timing. The full ICT entry sequence becomes:

```
BOS (trend established)
  → IDM identified (minor swing between BOS and entry zone)
  → IDM swept (liquidity taken from weak hands)
  → Price returns to OB + FVG confluence zone
  → Entry (highest conviction setup)
```

This adds a `has_inducement_cleared` boolean to the entry condition check
in `SMCStrategy._check_entry()`.

#### Pitfall: Fractal Strength Hierarchy

The existing `SwingDetector` uses a single configurable `strength` parameter.
IDM detection requires **two strength levels**: a higher-strength "primary"
detector and a lower-strength "secondary" detector for IDM candidates.

```python
# In SMCStrategy.__init__:
self._primary_swings = SwingDetector(strength=3)   # Major structure
self._idm_swings = SwingDetector(strength=1)        # IDM candidates (minor)
```

---

### 1.4 Kill Zones

#### Concept

Kill Zones are time windows during which ICT setups have statistically higher
probability due to institutional session overlap and news release times.

| Kill Zone         | EST                    | UTC          | Notes                          |
|-------------------|------------------------|--------------|--------------------------------|
| London Open       | 02:00 – 05:00          | 07:00–10:00  | Highest FX volume, strong moves|
| New York Open     | 07:00 – 10:00          | 12:00–15:00  | Overlap with London            |
| London Close      | 10:00 – 12:00          | 15:00–17:00  | Often reversal of London move  |
| NY Close          | 14:00 – 16:00          | 19:00–21:00  | Late US session, equity close  |
| Asian Kill Zone   | 20:00 – 23:00 (prev)   | 01:00–04:00  | Lower range, accumulation      |

For US equity indices (ES, NQ) the NY Open Kill Zone is the primary one.
For Forex the London Open is the strongest.

#### Algorithm

```python
from datetime import time, timezone
import pytz

class KillZone:
    """Defines a trading session window."""
    name: str
    start: time   # in UTC
    end: time     # in UTC
    markets: list[str]  # ["FX", "INDICES", "ALL"]

KILL_ZONES: list[KillZone] = [
    KillZone("London Open",  time(7, 0),  time(10, 0), ["FX", "INDICES"]),
    KillZone("New York Open", time(12, 0), time(15, 0), ["ALL"]),
    KillZone("London Close",  time(15, 0), time(17, 0), ["FX"]),
    KillZone("NY Close",      time(19, 0), time(21, 0), ["ALL"]),
]

def in_kill_zone(timestamp: datetime, zones: list[KillZone]) -> Optional[KillZone]:
    """Return the active KillZone for a given UTC timestamp, or None."""
    t = timestamp.astimezone(timezone.utc).time()
    for zone in zones:
        if zone.start <= t <= zone.end:
            return zone
    return None
```

#### Integration in Event-Driven Loop

Kill zone filtering is applied **inside `calculate_signals()`** as a gate:

```python
def calculate_signals(self, event: MarketEvent) -> Optional[SignalEvent]:
    # ... existing SMC pipeline ...

    # Kill zone gate: only generate signals during valid windows
    active_zone = in_kill_zone(event.timestamp, KILL_ZONES)
    if active_zone is None and self._params.get("kill_zone_filter", True):
        return None  # No trade outside kill zones

    # ... entry/exit checks ...
```

**Exit signals are NOT kill-zone-gated** — exits should always be honored
regardless of time.

#### Pitfall: Daily bar timeframe

Kill zones are meaningless on daily/weekly bars (each bar spans the entire day).
Apply kill zone filtering only for intraday timeframes (1m, 5m, 15m, 1h).

```python
INTRADAY_TIMEFRAMES = {"1m", "5m", "15m", "1h", "4h"}
if self._timeframe in INTRADAY_TIMEFRAMES:
    # apply kill zone filter
```

#### Pitfall: Timezone handling

The `MarketEvent.timestamp` in the current engine may be naive (no timezone).
For kill zone logic, timestamps MUST be timezone-aware. Add timezone
normalization in `DataHandler._load_data()` or in the kill zone checker.

```python
# Safe UTC normalization
if ts.tzinfo is None:
    ts = ts.replace(tzinfo=timezone.utc)
else:
    ts = ts.astimezone(timezone.utc)
```

#### Library

`pytz` (already available transitively via pandas) or `zoneinfo` (Python 3.9+
stdlib, preferred):

```python
from zoneinfo import ZoneInfo
EST = ZoneInfo("America/New_York")
```

---

### 1.5 Premium / Discount Zones

#### Concept

In any range defined by a swing high and swing low, the **equilibrium** is
the 50% midpoint. ICT defines:

- **Premium zone:** Price above 50% equilibrium (OTE: Optimal Trade Entry for
  shorts; prices are expensive here)
- **Discount zone:** Price below 50% equilibrium (OTE for longs; prices are cheap)

```
Range High (swing high)  ----  100%
                          ----   79% (upper Fib: 0.79 retracement)
                          ----   70.5% (Fib: 0.705 retracement)
Equilibrium               ----   50%
                          ----   38.2% (Fib: 0.382)
                          ----   20.5% (Fib: 0.205)
Range Low (swing low)    ----    0%
```

The **Optimal Trade Entry (OTE)** zone for longs = 61.8%–79% retracement
(discount). The OTE for shorts = 20.5%–38.2% retracement (premium).

#### Algorithm

```python
@dataclass(frozen=True)
class PremiumDiscountZone:
    range_high: Decimal
    range_low: Decimal
    equilibrium: Decimal    # (high + low) / 2
    ote_long_low: Decimal   # range_high - (range_high - range_low) * Decimal('0.79')
    ote_long_high: Decimal  # range_high - (range_high - range_low) * Decimal('0.618')
    ote_short_low: Decimal  # range_low + (range_high - range_low) * Decimal('0.205')
    ote_short_high: Decimal # range_low + (range_high - range_low) * Decimal('0.382')

def compute_premium_discount(
    swing_high: Decimal, swing_low: Decimal
) -> PremiumDiscountZone:
    span = swing_high - swing_low
    eq = (swing_high + swing_low) / Decimal('2')
    return PremiumDiscountZone(
        range_high=swing_high,
        range_low=swing_low,
        equilibrium=eq,
        ote_long_low=swing_high - span * Decimal('0.79'),
        ote_long_high=swing_high - span * Decimal('0.618'),
        ote_short_low=swing_low + span * Decimal('0.205'),
        ote_short_high=swing_low + span * Decimal('0.382'),
    )

def price_zone(price: Decimal, zone: PremiumDiscountZone) -> str:
    """Return 'premium', 'discount', or 'equilibrium'."""
    if price > zone.equilibrium:
        return "premium"
    elif price < zone.equilibrium:
        return "discount"
    return "equilibrium"
```

#### Integration

Premium/Discount is applied as an **entry filter**, not a signal generator.
It filters OB mitigations by position within the range:

```python
# Only take long OBs in the discount zone
if self._in_position == "" and trend == TrendState.UPTREND:
    pd_zone = compute_premium_discount(
        self._swing_detector.swing_highs[-1].price,
        self._swing_detector.swing_lows[-1].price,
    )
    if price_zone(event.close, pd_zone) != "discount":
        return None   # Not in discount — skip long entry
```

#### Pitfall: Range identification

The "relevant range" must be defined contextually. The most common approaches:

1. **Last BOS range:** From the swing low to the swing high that was broken by
   the most recent BOS (or CHOCH).
2. **Previous swing range:** Most recent confirmed swing high to most recent
   confirmed swing low.
3. **Session range:** High and low of the previous trading session (e.g.,
   prior day's range for intraday).

Each approach is valid for different timeframes. Start with option 2 (prior
swing range) as it uses already-available `SwingDetector` data.

#### Pitfall: Flat ranges

If `swing_high == swing_low` (perfectly flat range, rare but possible with
synthetic/forward-filled bars), division by zero occurs. Guard:

```python
if swing_high == swing_low:
    return None  # Cannot compute zones on zero-range
```

---

### 1.6 How ICT Concepts Interact with Existing SMC

The full institutional entry model becomes a **layered confluence filter**:

```
Layer 1 (Mandatory): BOS / CHOCH establishes trend (existing MarketStructureTracker)
Layer 2 (Mandatory): Entry in correct Premium/Discount zone (new)
Layer 3 (High weight): Active Order Block in entry zone (existing OrderBlockDetector)
Layer 4 (High weight): Overlapping FVG in OB zone (existing FVGTracker)
Layer 5 (Medium weight): Kill Zone active at entry time (new)
Layer 6 (Medium weight): Liquidity sweep precedes entry (new)
Layer 7 (Optional): Inducement cleared before entry (new)
```

A signal score can be computed:

```python
signal_strength = Decimal('0')
if layer_3_ob:      signal_strength += Decimal('0.25')
if layer_4_fvg:     signal_strength += Decimal('0.20')
if layer_5_killzone: signal_strength += Decimal('0.20')
if layer_6_sweep:   signal_strength += Decimal('0.20')
if layer_7_idm:     signal_strength += Decimal('0.15')
# Only enter if signal_strength >= threshold (e.g., 0.60)
```

This maps directly to the `SignalEvent.strength: Decimal` field already in
the event model.

---

## 2. Regime Detection

### 2.1 Overview

Regime detection classifies the current market environment so that the engine
can activate the optimal strategy for conditions (trending vs ranging, high vs
low volatility). All classifiers must be **rule-based** — no ML. They must
work inside the event loop, updating incrementally each bar using only
historical data.

---

### 2.2 ATR-Based Volatility Regimes

#### Concept

Volatility regimes are defined by comparing current ATR to its own rolling
average. When ATR is much higher than its average, the market is in a
high-volatility regime (breakouts work well). When ATR is much lower, the
market is in a low-volatility regime (mean-reversion and range-bound
strategies work better).

#### Algorithm

```python
from enum import Enum
from decimal import Decimal
from collections import deque

class VolatilityRegime(Enum):
    LOW = "LOW"       # ATR < mean_atr * low_threshold
    NORMAL = "NORMAL" # low_threshold <= ATR <= high_threshold
    HIGH = "HIGH"     # ATR > mean_atr * high_threshold

class ATRRegimeClassifier:
    """
    Computes rolling ATR and classifies volatility regime.

    Parameters
    ----------
    atr_period : int
        ATR lookback period. Default: 14.
    regime_lookback : int
        Bars to average ATR over for baseline. Default: 50.
    low_threshold : Decimal
        ATR < mean_atr * low_threshold => LOW regime. Default: 0.75.
    high_threshold : Decimal
        ATR > mean_atr * high_threshold => HIGH regime. Default: 1.50.
    """

    def __init__(
        self,
        atr_period: int = 14,
        regime_lookback: int = 50,
        low_threshold: Decimal = Decimal('0.75'),
        high_threshold: Decimal = Decimal('1.50'),
    ) -> None:
        self._atr_period = atr_period
        self._regime_lookback = regime_lookback
        self._low_threshold = low_threshold
        self._high_threshold = high_threshold
        self._atr_history: deque[Decimal] = deque(maxlen=regime_lookback)
        self._current_atr: Decimal = Decimal('0')
        self._regime: VolatilityRegime = VolatilityRegime.NORMAL

    def update(self, bar_buffer: list[MarketEvent]) -> VolatilityRegime:
        """Update ATR and return current regime. Call once per bar."""
        atr = self._compute_atr(bar_buffer)
        if atr > Decimal('0'):
            self._current_atr = atr
            self._atr_history.append(atr)

        if len(self._atr_history) < self._atr_period:
            return VolatilityRegime.NORMAL  # Not enough history

        mean_atr = sum(self._atr_history) / Decimal(str(len(self._atr_history)))

        if self._current_atr < mean_atr * self._low_threshold:
            self._regime = VolatilityRegime.LOW
        elif self._current_atr > mean_atr * self._high_threshold:
            self._regime = VolatilityRegime.HIGH
        else:
            self._regime = VolatilityRegime.NORMAL

        return self._regime

    def _compute_atr(self, bar_buffer: list[MarketEvent]) -> Decimal:
        """Wilder's ATR using simple average (consistent with SMCStrategy)."""
        if len(bar_buffer) < 2:
            return Decimal('0')
        period = min(self._atr_period, len(bar_buffer) - 1)
        tr_sum = Decimal('0')
        for i in range(-period, 0):
            bar = bar_buffer[i]
            prev_close = bar_buffer[i - 1].close
            tr = max(
                bar.high - bar.low,
                abs(bar.high - prev_close),
                abs(bar.low - prev_close),
            )
            tr_sum += tr
        return tr_sum / Decimal(str(period))

    @property
    def regime(self) -> VolatilityRegime:
        return self._regime

    @property
    def current_atr(self) -> Decimal:
        return self._current_atr
```

#### Pitfall: ATR Regime at Strategy Startup

During warmup (first N bars), the ATR history is short. Default to NORMAL
regime during warmup to avoid premature regime decisions. The `warmup_bars`
parameter in `SMCStrategy` already handles the signal-generation warmup, but
the regime classifier needs its own warmup guard.

---

### 2.3 ADX Trend Strength Classification

#### Concept

ADX (Average Directional Index) measures trend strength from 0–100, without
indicating direction. DI+ (Directional Indicator Plus) and DI- measure bullish
vs bearish pressure.

```
ADX < 20          => No trend (ranging market)
20 <= ADX < 25    => Weak trend (developing)
25 <= ADX < 40    => Moderate trend (trade with trend)
ADX >= 40         => Strong trend (momentum strategies)
```

Combined with VolatilityRegime for a 2D regime space:

```
              LOW Vol   NORMAL Vol   HIGH Vol
No Trend:    Ignore     Range       Fade extremes
Weak Trend:  Ignore     SMC         Breakout
Strong Trend: Ignore    Trend-follow Momentum
```

#### Algorithm

ADX computation from first principles (no external library needed, consistent
with the project's pure-Python math approach):

```python
class ADXClassifier:
    """
    Rule-based ADX trend strength classifier.

    ADX components (all Decimal):
        TR  = True Range (same as ATR input)
        +DM = max(high - prev_high, 0) if > max(prev_low - low, 0) else 0
        -DM = max(prev_low - low, 0) if > max(high - prev_high, 0) else 0
        Smoothed TR, +DM, -DM using Wilder's EMA (period = adx_period):
            Smoothed_X[0] = sum(X, period)
            Smoothed_X[n] = Smoothed_X[n-1] - (Smoothed_X[n-1] / period) + X[n]
        +DI = 100 * Smoothed(+DM) / Smoothed(TR)
        -DI = 100 * Smoothed(-DM) / Smoothed(TR)
        DX  = 100 * |+DI - -DI| / (+DI + -DI)
        ADX = Wilder EMA of DX over adx_period bars
    """

    def __init__(self, period: int = 14) -> None:
        self._period = period
        self._smoothed_tr = Decimal('0')
        self._smoothed_plus_dm = Decimal('0')
        self._smoothed_minus_dm = Decimal('0')
        self._smoothed_adx = Decimal('0')
        self._bar_count = 0
        self._adx: Decimal = Decimal('0')
        self._plus_di: Decimal = Decimal('0')
        self._minus_di: Decimal = Decimal('0')
        # Initialization phase: collect raw sums for first period
        self._init_tr: list[Decimal] = []
        self._init_plus_dm: list[Decimal] = []
        self._init_minus_dm: list[Decimal] = []
        self._init_dx: list[Decimal] = []
        self._initialized = False

    def update(self, bar: MarketEvent, prev_bar: MarketEvent) -> Decimal:
        """Update ADX with current bar. Returns current ADX value."""
        tr = max(
            bar.high - bar.low,
            abs(bar.high - prev_bar.close),
            abs(bar.low - prev_bar.close),
        )
        up_move = bar.high - prev_bar.high
        down_move = prev_bar.low - bar.low

        plus_dm = up_move if up_move > down_move and up_move > Decimal('0') else Decimal('0')
        minus_dm = down_move if down_move > up_move and down_move > Decimal('0') else Decimal('0')

        if not self._initialized:
            self._init_tr.append(tr)
            self._init_plus_dm.append(plus_dm)
            self._init_minus_dm.append(minus_dm)

            if len(self._init_tr) == self._period:
                # First smoothed values = simple sum of first period
                self._smoothed_tr = sum(self._init_tr)
                self._smoothed_plus_dm = sum(self._init_plus_dm)
                self._smoothed_minus_dm = sum(self._init_minus_dm)
                self._initialized = True
                return self._compute_adx()
            return Decimal('0')

        # Wilder's smoothing: Smooth = Smooth - Smooth/period + new
        p = Decimal(str(self._period))
        self._smoothed_tr = self._smoothed_tr - self._smoothed_tr / p + tr
        self._smoothed_plus_dm = self._smoothed_plus_dm - self._smoothed_plus_dm / p + plus_dm
        self._smoothed_minus_dm = self._smoothed_minus_dm - self._smoothed_minus_dm / p + minus_dm
        return self._compute_adx()

    def _compute_adx(self) -> Decimal:
        if self._smoothed_tr == Decimal('0'):
            return Decimal('0')
        p = Decimal(str(self._period))
        self._plus_di = Decimal('100') * self._smoothed_plus_dm / self._smoothed_tr
        self._minus_di = Decimal('100') * self._smoothed_minus_dm / self._smoothed_tr
        di_sum = self._plus_di + self._minus_di
        if di_sum == Decimal('0'):
            return Decimal('0')
        dx = Decimal('100') * abs(self._plus_di - self._minus_di) / di_sum
        # Wilder-smooth ADX
        if self._adx == Decimal('0'):
            self._adx = dx
        else:
            self._adx = (self._adx * (p - Decimal('1')) + dx) / p
        return self._adx

    def classify(self) -> str:
        """Return trend classification string."""
        if self._adx < Decimal('20'):
            return "RANGING"
        elif self._adx < Decimal('25'):
            return "WEAK_TREND"
        elif self._adx < Decimal('40'):
            return "TRENDING"
        else:
            return "STRONG_TREND"

    @property
    def adx(self) -> Decimal:
        return self._adx

    @property
    def plus_di(self) -> Decimal:
        return self._plus_di

    @property
    def minus_di(self) -> Decimal:
        return self._minus_di
```

#### Pitfall: ADX is a lagging indicator

ADX crosses ADX=25 roughly 2–5 bars after the trend starts on daily data, and
proportionally later on higher timeframes. For regime-switching, use ADX
direction (rising vs falling) in addition to absolute level:

```python
# Trend is strengthening if ADX rising and > 20
trend_strengthening = adx_now > adx_prev and adx_now > Decimal('20')
```

---

### 2.4 Simple Regime Classifier (Combined)

Combine ATR volatility and ADX trend strength into one `MarketRegime` dataclass:

```python
from dataclasses import dataclass
from enum import Enum

class RegimeType(Enum):
    STRONG_TREND   = "STRONG_TREND"    # ADX >= 40, vol NORMAL or HIGH
    MODERATE_TREND = "MODERATE_TREND"  # ADX 25-40, vol NORMAL
    WEAK_TREND     = "WEAK_TREND"      # ADX 20-25
    RANGING_LOW    = "RANGING_LOW"     # ADX < 20, vol LOW
    RANGING_NORMAL = "RANGING_NORMAL"  # ADX < 20, vol NORMAL
    CHOPPY         = "CHOPPY"          # ADX < 20, vol HIGH (avoid trading)

@dataclass(frozen=True)
class MarketRegime:
    regime_type: RegimeType
    adx: Decimal
    adx_trend: str          # "rising" or "falling"
    vol_regime: VolatilityRegime
    current_atr: Decimal
    plus_di: Decimal
    minus_di: Decimal
    bullish_pressure: bool  # plus_di > minus_di

class RegimeClassifier:
    """Combined regime classifier: ATR volatility + ADX trend strength."""

    def __init__(
        self,
        atr_period: int = 14,
        adx_period: int = 14,
        regime_lookback: int = 50,
    ) -> None:
        self._atr_clf = ATRRegimeClassifier(atr_period, regime_lookback)
        self._adx_clf = ADXClassifier(adx_period)
        self._prev_adx = Decimal('0')

    def update(self, event: MarketEvent, bar_buffer: list[MarketEvent]) -> MarketRegime:
        """Update both classifiers and return the combined regime."""
        vol_regime = self._atr_clf.update(bar_buffer)
        if len(bar_buffer) >= 2:
            adx = self._adx_clf.update(event, bar_buffer[-2])
        else:
            adx = Decimal('0')

        adx_trend = "rising" if adx > self._prev_adx else "falling"
        self._prev_adx = adx

        regime_type = self._classify(adx, vol_regime)

        return MarketRegime(
            regime_type=regime_type,
            adx=adx,
            adx_trend=adx_trend,
            vol_regime=vol_regime,
            current_atr=self._atr_clf.current_atr,
            plus_di=self._adx_clf.plus_di,
            minus_di=self._adx_clf.minus_di,
            bullish_pressure=self._adx_clf.plus_di > self._adx_clf.minus_di,
        )

    def _classify(self, adx: Decimal, vol: VolatilityRegime) -> RegimeType:
        if adx >= Decimal('40') and vol in (VolatilityRegime.NORMAL, VolatilityRegime.HIGH):
            return RegimeType.STRONG_TREND
        elif adx >= Decimal('25') and vol == VolatilityRegime.NORMAL:
            return RegimeType.MODERATE_TREND
        elif adx >= Decimal('20'):
            return RegimeType.WEAK_TREND
        elif vol == VolatilityRegime.LOW:
            return RegimeType.RANGING_LOW
        elif vol == VolatilityRegime.HIGH:
            return RegimeType.CHOPPY
        else:
            return RegimeType.RANGING_NORMAL
```

---

### 2.5 Dynamic Strategy Selection Based on Regime

#### Pattern: Strategy Router with Regime Gate

The existing `PortfolioRouter` (`src/portfolio_router.py`) routes signals from
multiple strategies. For regime-based selection, a simpler `RegimeRouter`
wraps a single strategy and gates it based on regime:

```python
class RegimeGatedStrategy(BaseStrategy):
    """
    Wraps another strategy and only generates signals in approved regimes.

    Parameters (via params dict)
    ----------------------------
    allowed_regimes : list[str]
        Regime types where signals are passed through.
        e.g. ["STRONG_TREND", "MODERATE_TREND"]
    """

    def __init__(
        self,
        inner_strategy: BaseStrategy,
        allowed_regimes: list[RegimeType],
        atr_period: int = 14,
        adx_period: int = 14,
    ) -> None:
        super().__init__(inner_strategy.symbol, inner_strategy.timeframe)
        self._inner = inner_strategy
        self._allowed = set(allowed_regimes)
        self._regime_clf = RegimeClassifier(atr_period, adx_period)

    def calculate_signals(self, event: MarketEvent) -> Optional[SignalEvent]:
        self.update_buffer(event)
        regime = self._regime_clf.update(event, self._bar_buffer)

        # Always delegate to inner for buffer management
        inner_signal = self._inner.calculate_signals(event)

        if regime.regime_type not in self._allowed:
            return None  # Gate: suppress signals in wrong regime

        return inner_signal
```

#### Regime-to-Strategy Mapping

| Regime          | Preferred Strategy           | Notes                                |
|-----------------|------------------------------|--------------------------------------|
| STRONG_TREND    | Breakout, SMC trend-follow   | Wide stops, trail                    |
| MODERATE_TREND  | SMC (OB+FVG), Trend-follow   | Standard ICT setups                  |
| WEAK_TREND      | SMC with tighter filters     | Require sweep + IDM confirmation     |
| RANGING_NORMAL  | Reversal at range extremes   | Support/resistance bounces           |
| RANGING_LOW     | Skip / reduce size           | Low ATR = tight stops, bad R:R       |
| CHOPPY          | No trading                   | High vol + no trend = random moves   |

#### Integration in the Event-Driven Loop

The `BacktestEngine.run()` loop does not change. The `RegimeGatedStrategy`
is passed in as the `strategy` parameter. All regime logic is internal to
the strategy class — the engine remains dumb.

```python
smc = SMCStrategy(symbol="NQ", timeframe="1h")
gated_smc = RegimeGatedStrategy(
    inner_strategy=smc,
    allowed_regimes=[RegimeType.MODERATE_TREND, RegimeType.STRONG_TREND],
)
engine = create_engine(data_handler, gated_smc, ...)
```

---

### 2.6 Integration into the Event-Driven Backtest Loop

The regime classifier must be **incremental** (update one bar at a time using
the rolling `bar_buffer`). Key invariants:

1. `RegimeClassifier.update()` is called with `event` and the current
   `bar_buffer` at the START of `calculate_signals()`, before any signal logic.
2. The regime computed at bar N is based on bars 0..N (no lookahead).
3. Regime state persists across bars (the classifier maintains its smoothed
   state in instance variables — Wilder's EMA requires the prior value).
4. The `MarketRegime` object is a frozen dataclass (immutable result), but the
   classifier itself is stateful.

#### Pitfall: State Leakage in Parameter Sweeps

The optimization engine (`walk_forward.py`, `sensitivity.py`) creates multiple
engine instances per parameter combination. Each instance must get a fresh
`RegimeClassifier`. Since the regime classifier is embedded in the strategy,
and the strategy is instantiated fresh per sweep via `create_engine()`, this
is automatically safe.

---

## 3. Advanced Risk Management

### 3.1 Overview

v2.0's Portfolio uses fixed fractional sizing (10% of equity / price in
`BacktestEngine._calculate_order_quantity()`). v3.0 replaces this with a
full risk management module that computes position size from multiple
constraints simultaneously and enforces portfolio-level heat limits.

All computation uses `decimal.Decimal` with string constructor.

---

### 3.2 Kelly Criterion

#### Concept

Kelly Criterion computes the theoretically optimal fraction of equity to risk
per trade to maximize long-run geometric growth:

```
Kelly Fraction (f*) = W - (1 - W) / R

Where:
    W = win rate (fraction, e.g., 0.55 for 55%)
    R = average win / average loss (reward-to-risk ratio)
```

Full Kelly is too aggressive for real trading (produces severe drawdowns from
variance). Practitioners use **Half Kelly** (50% of the Kelly fraction) or
**Quarter Kelly** as safer alternatives.

#### Implementation

```python
from decimal import Decimal

def kelly_fraction(
    win_rate: Decimal,
    avg_win: Decimal,
    avg_loss: Decimal,
    kelly_factor: Decimal = Decimal('0.5'),  # 0.5 = half Kelly
) -> Decimal:
    """
    Compute Kelly-adjusted position size fraction.

    Parameters
    ----------
    win_rate : Decimal
        Historical win rate as fraction (0.0 to 1.0).
    avg_win : Decimal
        Average winning trade PnL (absolute, positive).
    avg_loss : Decimal
        Average losing trade PnL (absolute, positive).
    kelly_factor : Decimal
        Multiplier on raw Kelly (default 0.5 = half Kelly).

    Returns
    -------
    Decimal
        Fraction of equity to risk (e.g., 0.02 = 2%).
        Clamped to [0, max_fraction].
    """
    if avg_loss <= Decimal('0') or win_rate <= Decimal('0'):
        return Decimal('0')

    r = avg_win / avg_loss
    lose_rate = Decimal('1') - win_rate
    raw_kelly = win_rate - lose_rate / r

    if raw_kelly <= Decimal('0'):
        return Decimal('0')  # Negative Kelly = don't trade this system

    adjusted = raw_kelly * kelly_factor
    # Hard cap at 25% per trade (even half Kelly can be extreme with good stats)
    return min(adjusted, Decimal('0.25'))
```

#### Integration

Kelly is computed from the **live fill_log** (rolling lookback window, e.g.,
last 50 trades) and recalculated periodically (e.g., every 20 trades).

```python
class KellyPositionSizer:
    def __init__(
        self,
        lookback_trades: int = 50,
        kelly_factor: Decimal = Decimal('0.5'),
        min_trades: int = 20,  # Minimum trades before Kelly is used
        fallback_fraction: Decimal = Decimal('0.01'),  # Used during warmup
    ):
        ...

    def compute_fraction(self, fill_log: list[FillEvent]) -> Decimal:
        """Compute Kelly fraction from recent fill history."""
        recent = self._get_recent_trades(fill_log)  # pair fills into round trips
        if len(recent) < self._min_trades:
            return self._fallback_fraction  # Not enough data
        win_rate, avg_win, avg_loss = self._compute_stats(recent)
        return kelly_fraction(win_rate, avg_win, avg_loss, self._kelly_factor)
```

#### Pitfall: Circular Dependency

Kelly depends on past trade results, but trade results depend on position
sizing. During the initial warmup period (first N trades), use fixed fractional
as a fallback. Never compute Kelly on fewer than 20–30 trades — with small
samples, Kelly estimates are unstable.

#### Pitfall: Kelly Blowing Up with Perfect Stats

A strategy with 100% win rate and large R produces extreme Kelly fractions.
Always cap at 25% even after Kelly adjustment. In practice, cap the raw kelly
at 0.5 (50% equity) before applying the kelly_factor.

---

### 3.3 Fixed Fractional Risk per Trade

This is the workhorse position sizing method. The existing
`Portfolio.calculate_position_size()` implements the core formula but is not
integrated into `BacktestEngine._calculate_order_quantity()` (which uses a
naive 10% of equity / price).

**The correct formula:**

```
risk_amount   = equity * risk_fraction           (e.g., equity * 0.01 = 1% risk)
position_size = risk_amount / stop_distance      (stop_distance in price units)
```

**Implementation in BacktestEngine:**

```python
def _calculate_order_quantity(
    self,
    bar: MarketEvent,
    stop_distance: Decimal,  # price units from entry to stop
    risk_fraction: Decimal = Decimal('0.01'),  # 1% per trade
) -> Decimal:
    """Fixed fractional sizing: risk exactly risk_fraction of equity."""
    equity_log = self._portfolio.equity_log
    equity = equity_log[-1]["equity"] if equity_log else self._portfolio.cash

    if bar.close <= Decimal('0') or stop_distance <= Decimal('0'):
        return Decimal('0')

    risk_amount = equity * risk_fraction
    raw_qty = risk_amount / stop_distance

    # Floor to appropriate precision
    # Stocks: floor to integer shares
    # Forex lots: floor to 0.01 lot precision
    return raw_qty.quantize(Decimal('1'), rounding=ROUND_DOWN)
```

**The stop_distance problem:** The `SignalEvent` does not currently carry stop
price information. To pass stop distance from strategy to engine, either:

1. **Extend SignalEvent** to include optional `stop_price: Optional[Decimal]`
   (cleanest, but changes the frozen dataclass API)
2. **Store stop price in strategy state** and expose it via a property:
   `strategy.last_stop_price` (looser coupling, works without API change)
3. **Use ATR-multiple as proxy:** `stop_distance = current_atr * stop_atr_mult`
   (approximation, avoids API change entirely)

Recommended approach for v3.0: option 3 (ATR-based stop proxy) for minimal
API disruption, document option 1 for v4.0.

---

### 3.4 Portfolio Heat

**Portfolio Heat** is the total percentage of equity at risk across all open
positions simultaneously.

```
portfolio_heat = sum(
    open_position_risk[symbol]
    for symbol in open_positions
)

open_position_risk[symbol] = (
    position_size[symbol] * stop_distance[symbol] / equity
)
```

If heat exceeds the maximum (e.g., 6% for a 3-position max at 2% each),
new entries are blocked until some positions close.

#### Implementation

```python
class PortfolioHeatMonitor:
    """
    Tracks open risk across all positions.

    Requires tracking stop prices per open position — not stored by
    the current Portfolio. A companion dict is maintained by the strategy
    or a new RiskManager component.
    """

    def __init__(self, max_heat: Decimal = Decimal('0.06')) -> None:
        self._max_heat = max_heat
        self._stop_prices: dict[str, Decimal] = {}  # symbol -> stop price
        self._entry_prices: dict[str, Decimal] = {}

    def register_trade(
        self,
        symbol: str,
        entry_price: Decimal,
        stop_price: Decimal,
        quantity: Decimal,
    ) -> None:
        self._stop_prices[symbol] = stop_price
        self._entry_prices[symbol] = entry_price

    def close_trade(self, symbol: str) -> None:
        self._stop_prices.pop(symbol, None)
        self._entry_prices.pop(symbol, None)

    def current_heat(
        self,
        positions: dict[str, 'Position'],
        equity: Decimal,
    ) -> Decimal:
        """Compute total portfolio heat as fraction of equity."""
        total_heat = Decimal('0')
        for symbol, pos in positions.items():
            if pos.quantity <= Decimal('0'):
                continue
            stop = self._stop_prices.get(symbol)
            if stop is None:
                continue
            entry = self._entry_prices.get(symbol)
            if entry is None:
                continue
            risk_per_unit = abs(entry - stop)
            position_risk = risk_per_unit * pos.quantity
            heat = position_risk / equity if equity > Decimal('0') else Decimal('0')
            total_heat += heat
        return total_heat

    def can_add_position(self, new_heat: Decimal, equity: Decimal) -> bool:
        """Returns True if adding a new position would not exceed max heat."""
        current = self.current_heat(...)  # requires current positions context
        return (current + new_heat) <= self._max_heat
```

---

### 3.5 Drawdown-Based Position Scaling

During a drawdown, reduce position size to preserve capital and avoid
compounding losses. Common implementation: the **Drawdown Factor**.

```
drawdown_pct = (peak_equity - current_equity) / peak_equity

drawdown_factor = max(
    min_scale,
    1 - (drawdown_pct / max_allowed_dd) * scale_aggressiveness
)

effective_risk = base_risk * drawdown_factor
```

Example: If max allowed drawdown = 20% and current drawdown = 10%:
`drawdown_factor = 1 - (0.10 / 0.20) * 1.0 = 0.50` → trade at half size.

```python
def drawdown_scale_factor(
    equity: Decimal,
    peak_equity: Decimal,
    max_drawdown: Decimal = Decimal('0.20'),      # 20% max
    min_scale: Decimal = Decimal('0.25'),          # minimum 25% of normal size
    aggressiveness: Decimal = Decimal('1.0'),      # linear scaling
) -> Decimal:
    """Return a scale factor in [min_scale, 1.0]."""
    if peak_equity <= Decimal('0'):
        return Decimal('1.0')
    dd_pct = (peak_equity - equity) / peak_equity
    if dd_pct <= Decimal('0'):
        return Decimal('1.0')  # At peak — full size

    raw_factor = Decimal('1') - (dd_pct / max_drawdown) * aggressiveness
    return max(min_scale, min(Decimal('1'), raw_factor))
```

#### Integration

The `BacktestEngine` already logs equity via `Portfolio.equity_log`. The
peak equity can be tracked via a running max of `equity_log`:

```python
# In BacktestEngine.run():
peak_equity = portfolio.cash  # initialize to starting capital

for bar in data_handler.stream_bars():
    current_equity = portfolio.equity_log[-1]["equity"] if portfolio.equity_log else portfolio.cash
    peak_equity = max(peak_equity, current_equity)
    dd_factor = drawdown_scale_factor(current_equity, peak_equity)
    # pass dd_factor to _calculate_order_quantity() to adjust size
```

---

### 3.6 Maximum Concurrent Positions

Limit the number of simultaneously open positions to prevent excessive
concentration risk.

```python
class RiskManager:
    """Centralizes all position sizing and risk constraint enforcement."""

    def __init__(
        self,
        base_risk_pct: Decimal = Decimal('0.01'),     # 1% per trade
        max_heat_pct: Decimal = Decimal('0.06'),       # 6% total portfolio heat
        max_concurrent: int = 3,                       # max open positions
        max_daily_risk_pct: Decimal = Decimal('0.03'), # 3% max risk per day
        kelly_enabled: bool = False,
        kelly_lookback: int = 50,
        kelly_factor: Decimal = Decimal('0.5'),
    ) -> None:
        self._base_risk_pct = base_risk_pct
        self._max_heat_pct = max_heat_pct
        self._max_concurrent = max_concurrent
        self._max_daily_risk_pct = max_daily_risk_pct
        self._kelly_enabled = kelly_enabled
        self._heat_monitor = PortfolioHeatMonitor(max_heat_pct)
        self._kelly_sizer = KellyPositionSizer(kelly_lookback, kelly_factor) if kelly_enabled else None
        self._daily_risk_used: Decimal = Decimal('0')
        self._current_day: Optional[date] = None

    def can_trade(
        self,
        portfolio: Portfolio,
        equity: Decimal,
        current_bar: MarketEvent,
    ) -> tuple[bool, str]:
        """Master gate: returns (can_trade, reason)."""
        open_count = sum(
            1 for pos in portfolio.positions.values()
            if pos.quantity > Decimal('0')
        )
        if open_count >= self._max_concurrent:
            return False, f"Max concurrent positions ({self._max_concurrent}) reached"

        # Reset daily risk tracker on new day
        bar_date = current_bar.timestamp.date()
        if bar_date != self._current_day:
            self._daily_risk_used = Decimal('0')
            self._current_day = bar_date

        if self._daily_risk_used >= equity * self._max_daily_risk_pct:
            return False, "Daily risk limit reached"

        return True, "OK"

    def compute_quantity(
        self,
        equity: Decimal,
        peak_equity: Decimal,
        stop_distance: Decimal,
        fill_log: list[FillEvent],
    ) -> Decimal:
        """Compute final position size after all adjustments."""
        # Base risk fraction
        if self._kelly_enabled and self._kelly_sizer:
            risk_frac = self._kelly_sizer.compute_fraction(fill_log)
        else:
            risk_frac = self._base_risk_pct

        # Drawdown scaling
        dd_factor = drawdown_scale_factor(equity, peak_equity)
        effective_risk = risk_frac * dd_factor

        # Risk amount and raw size
        risk_amount = equity * effective_risk
        if stop_distance <= Decimal('0'):
            return Decimal('0')

        raw_qty = risk_amount / stop_distance
        return raw_qty.quantize(Decimal('1'), rounding=ROUND_DOWN)

    def register_fill(
        self,
        fill: FillEvent,
        stop_price: Decimal,
        equity: Decimal,
    ) -> None:
        """Track opened positions for heat and daily risk."""
        risk = abs(fill.fill_price - stop_price) * fill.quantity
        self._daily_risk_used += risk
        self._heat_monitor.register_trade(
            fill.symbol, fill.fill_price, stop_price, fill.quantity
        )
```

#### Placement in Architecture

`RiskManager` sits between `BacktestEngine` and `Portfolio`. The engine
passes the `RiskManager` at construction. The signal-to-order conversion
calls `risk_manager.can_trade()` and `risk_manager.compute_quantity()`.

```python
# In BacktestEngine._signal_to_order():
can_enter, reason = self._risk_manager.can_trade(self._portfolio, equity, bar)
if not can_enter:
    return None

stop_atr_mult = Decimal(str(self._strategy.params.get('stop_atr_mult', 2.0)))
stop_dist = self._strategy.current_atr * stop_atr_mult  # requires current_atr property
qty = self._risk_manager.compute_quantity(equity, self._peak_equity, stop_dist, fill_log)
```

---

### 3.7 Risk Per Trade vs Risk Per Day

The `RiskManager._max_daily_risk_pct` caps the total risk for a given day:

```python
daily_risk_used += abs(entry_price - stop_price) * quantity
if daily_risk_used >= equity * max_daily_risk_pct:
    block_new_trades_for_rest_of_day()
```

This prevents the common pathology of a strategy opening 10 losing trades in
a row on a bad day and blowing the account.

#### Pitfall: Daily Reset Logic

On daily bars, each bar IS one day — reset after every bar. On intraday bars,
reset when the date of `event.timestamp.date()` changes from the previous bar's
date. The `RiskManager.can_trade()` method handles this via `_current_day`
comparison.

---

## 4. Multi-Asset Foundation

### 4.1 Overview

Running multiple symbols through the same event-driven engine requires:
1. A way to interleave MarketEvents from multiple symbols chronologically
2. A Portfolio that tracks positions and heat across all symbols
3. Correlation awareness to avoid over-concentration in correlated assets
4. Per-asset position limits
5. Correct handling of different trading hours

The existing `DataHandler.align_multi_symbol()` already handles data alignment.
What is missing is engine-level multi-symbol dispatch and portfolio-level
cross-asset risk management.

---

### 4.2 Multi-Symbol Event Dispatch

#### The Core Problem

The existing `BacktestEngine.run()` consumes a single `DataHandler` generator.
For multi-asset, bars from multiple symbols must be chronologically merged into
a single stream and dispatched to the appropriate strategy.

#### Algorithm: Heap-Based Chronological Merge

```python
import heapq
from typing import Iterator

def merge_bars(
    handlers: dict[str, DataHandler]
) -> Iterator[MarketEvent]:
    """
    Merge multiple DataHandler generators into a single chronologically
    sorted stream of MarketEvents.

    Uses a min-heap keyed on (timestamp, symbol) to merge efficiently.
    Time complexity: O(N log K) where N = total bars, K = number of symbols.
    """
    # Initialize heap with first bar from each handler
    heap: list[tuple] = []
    generators = {
        sym: handler.stream_bars()
        for sym, handler in handlers.items()
    }

    for sym, gen in generators.items():
        try:
            bar = next(gen)
            # Heap key: (timestamp, symbol) — symbol breaks timestamp ties
            heapq.heappush(heap, (bar.timestamp, sym, bar, gen))
        except StopIteration:
            pass

    while heap:
        ts, sym, bar, gen = heapq.heappop(heap)
        yield bar
        try:
            next_bar = next(gen)
            heapq.heappush(heap, (next_bar.timestamp, sym, next_bar, gen))
        except StopIteration:
            pass  # This generator exhausted
```

#### Note on datetime comparison

`datetime` objects are comparable with `<`, `>`, `==`. However, if timestamps
are timezone-naive for some symbols and timezone-aware for others, the
comparison raises `TypeError`. Normalize all timestamps to UTC-naive in
`DataHandler.stream_bars()` before the merge.

#### Alternative: Synchronous Aligned Mode

For daily data, `DataHandler.align_multi_symbol()` already produces aligned
generators. In this mode, one bar from each symbol has the same timestamp —
dispatch each synchronously per timestamp:

```python
# For daily multi-asset: process all symbols for timestamp T before advancing
aligned = DataHandler.align_multi_symbol(handlers)
for date in sorted_dates:
    for sym in symbols:
        bar = aligned[sym][date]
        strategy[sym].calculate_signals(bar)
```

---

### 4.3 Multi-Symbol BacktestEngine

```python
class MultiAssetBacktestEngine:
    """
    Extends BacktestEngine for multiple symbols.

    Key changes vs single-asset engine:
    - strategies: dict[str, BaseStrategy] — one per symbol
    - execution: dict[str, ExecutionHandler] — one per symbol
    - portfolio: shared Portfolio (tracks positions across all symbols)
    - risk_manager: shared RiskManager (enforces portfolio-level heat)
    """

    def __init__(
        self,
        data_handlers: dict[str, DataHandler],
        strategies: dict[str, BaseStrategy],
        portfolio: Portfolio,
        execution_handlers: dict[str, ExecutionHandler],
        risk_manager: Optional[RiskManager] = None,
    ) -> None:
        self._handlers = data_handlers
        self._strategies = strategies
        self._portfolio = portfolio
        self._execution = execution_handlers
        self._risk_manager = risk_manager
        self._event_log: list = []

    def run(self) -> BacktestResult:
        total_bars = 0

        for bar in merge_bars(self._handlers):
            total_bars += 1
            sym = bar.symbol

            # Process pending orders for this symbol
            fills = self._execution[sym].process_bar(bar)
            for fill in fills:
                self._portfolio.process_fill(fill)
                self._event_log.append(fill)

            # Margin check across all symbols
            prices = self._get_all_last_prices()
            prices[sym] = bar.close
            to_liquidate = self._portfolio.check_margin(prices)
            for liq_sym in to_liquidate:
                liq_price = prices.get(liq_sym, bar.close)
                liq_fill = self._portfolio.force_liquidate(liq_sym, liq_price)
                if liq_fill:
                    self._event_log.append(liq_fill)

            # Generate signal
            strategy = self._strategies.get(sym)
            if strategy:
                signal = strategy.calculate_signals(bar)
                if signal:
                    order = self._signal_to_order(signal, bar)
                    if order:
                        self._execution[sym].submit_order(order)
                        self._event_log.append(order)

            # Update equity
            self._portfolio.update_equity(bar)

        return BacktestResult(
            equity_log=self._portfolio.equity_log,
            fill_log=self._portfolio.fill_log,
            event_log=self._event_log,
            final_equity=self._portfolio.equity_log[-1]["equity"] if self._portfolio.equity_log else Decimal('0'),
            total_bars=total_bars,
        )
```

---

### 4.4 Cross-Asset Correlation Computation

#### Purpose

High positive correlation between two assets means holding both simultaneously
doubles effective exposure to the same risk factor. The correlation monitor
detects this and either blocks new positions in correlated assets or halves the
size of the second position.

#### Algorithm: Rolling Pearson Correlation

```python
from decimal import Decimal
from collections import deque
import math

class CorrelationMonitor:
    """
    Computes pairwise rolling Pearson correlation between symbol returns.

    Implementation uses only stdlib (no numpy/scipy) for Decimal compatibility.
    Note: intermediate computation uses float (correlation coefficients do
    not require Decimal precision — they are dimensionless ratios).

    Parameters
    ----------
    lookback : int
        Number of bars for rolling window. Default: 60.
    high_correlation_threshold : float
        |r| >= threshold means "highly correlated". Default: 0.70.
    """

    def __init__(
        self,
        lookback: int = 60,
        high_correlation_threshold: float = 0.70,
    ) -> None:
        self._lookback = lookback
        self._threshold = high_correlation_threshold
        # returns[symbol] = deque of float returns (bar-to-bar pct changes)
        self._returns: dict[str, deque[float]] = {}
        self._last_close: dict[str, float] = {}

    def update(self, bar: MarketEvent) -> None:
        """Add the latest bar's return for a symbol."""
        sym = bar.symbol
        close = float(bar.close)

        if sym not in self._returns:
            self._returns[sym] = deque(maxlen=self._lookback)
            self._last_close[sym] = close
            return

        prev = self._last_close[sym]
        if prev != 0:
            ret = (close - prev) / prev
            self._returns[sym].append(ret)
        self._last_close[sym] = close

    def correlation(self, sym_a: str, sym_b: str) -> Optional[float]:
        """Compute Pearson r between sym_a and sym_b returns."""
        returns_a = list(self._returns.get(sym_a, []))
        returns_b = list(self._returns.get(sym_b, []))

        n = min(len(returns_a), len(returns_b))
        if n < 10:  # Need minimum history
            return None

        # Align to same length (most recent N)
        a = returns_a[-n:]
        b = returns_b[-n:]

        mean_a = sum(a) / n
        mean_b = sum(b) / n
        dev_a = [x - mean_a for x in a]
        dev_b = [x - mean_b for x in b]

        cov = sum(da * db for da, db in zip(dev_a, dev_b)) / n
        std_a = math.sqrt(sum(d**2 for d in dev_a) / n)
        std_b = math.sqrt(sum(d**2 for d in dev_b) / n)

        if std_a == 0 or std_b == 0:
            return None

        return cov / (std_a * std_b)

    def are_correlated(self, sym_a: str, sym_b: str) -> bool:
        """Return True if |correlation| >= threshold."""
        r = self.correlation(sym_a, sym_b)
        return r is not None and abs(r) >= self._threshold

    def correlation_scale_factor(self, sym_a: str, sym_b: str) -> float:
        """
        Returns position size multiplier for sym_b when sym_a is already open.

        1.0   = uncorrelated (full size)
        0.5   = r = threshold (half size)
        0.0   = r = 1.0 (no additional position — same direction)
        """
        r = self.correlation(sym_a, sym_b)
        if r is None:
            return 1.0
        abs_r = abs(r)
        if abs_r < self._threshold:
            return 1.0
        # Linear reduction from threshold to 1.0
        return max(0.0, 1.0 - (abs_r - self._threshold) / (1.0 - self._threshold))
```

#### Pitfall: Float for Correlation is Acceptable

Correlation coefficients are dimensionless statistical ratios, not monetary
values. Using `float` for their computation is acceptable — the precision
difference between `float` and `Decimal` is irrelevant for correlation
estimates (which are noisy by nature). Only PnL, prices, and balances require
`Decimal`.

---

### 4.5 Per-Asset Position Limits

```python
@dataclass
class AssetRiskProfile:
    """Per-symbol risk configuration."""
    symbol: str
    max_position_pct: Decimal  # Max % of equity in this symbol
    max_concurrent: int        # Max open positions for this symbol
    risk_per_trade_pct: Decimal  # Override base risk % for this symbol

# Example configuration
ASSET_PROFILES = {
    "NQ": AssetRiskProfile("NQ",
        max_position_pct=Decimal('0.20'),
        max_concurrent=1,
        risk_per_trade_pct=Decimal('0.01'),
    ),
    "ES": AssetRiskProfile("ES",
        max_position_pct=Decimal('0.20'),
        max_concurrent=1,
        risk_per_trade_pct=Decimal('0.01'),
    ),
    "EURUSD": AssetRiskProfile("EURUSD",
        max_position_pct=Decimal('0.10'),
        max_concurrent=2,
        risk_per_trade_pct=Decimal('0.005'),
    ),
}
```

The `RiskManager.can_trade()` method checks the `AssetRiskProfile` for the
symbol in `bar.symbol` before allowing a new position.

---

### 4.6 Data Synchronization Across Timeframes and Symbols

#### Problem: Different Bar Counts per Symbol

NYSE equities: 252 trading days/year, 6.5 hours/day
Forex: 261 trading days/year (no major exchange), 24 hours/day
Futures: vary by contract (ES, NQ trade ~23h/day)
Crypto: 365 days/year, 24 hours/day

When aligning daily bars, a Monday for NYSE is also a Monday for Forex but
Forex has bars on Saturday too. The existing `DataHandler.align_multi_symbol()`
uses forward-fill for missing dates — this is correct but has a subtle issue:

**A forward-filled bar has no trading activity.** Any signal generated on a
forward-filled bar is unrealistic (no liquidity). Filter:

```python
# In multi-asset strategy, skip signal generation on forward-filled bars
if bar.volume == 1:  # _FILL_VOLUME constant from data_handler.py
    return None  # Skip filled bars
```

#### Multi-Timeframe Data Alignment

The v2.0 engine has no native multi-timeframe support. For v3.0, the simplest
approach is **indicator-only multi-timeframe**: compute HTF (higher timeframe)
indicators from the LTF (lower timeframe) buffer by resampling.

```python
def resample_to_higher_tf(
    bar_buffer: list[MarketEvent],
    source_tf: str,   # e.g., "15m"
    target_tf: str,   # e.g., "1h"
) -> list[MarketEvent]:
    """
    Aggregate LTF bars into HTF bars from the rolling buffer.
    Used for HTF context (trend, OB, regime) without separate data feed.

    Grouping logic:
    - "1h" from "15m": group every 4 bars by hour
    - "4h" from "1h": group every 4 bars by 4-hour period
    - "1d" from "1h": group every 6.5 bars (partial days at buffer boundaries)

    Returns HTF bars (oldest first).
    No lookahead: only bars already in the buffer are used.
    """
    if not bar_buffer:
        return []

    # Group by HTF period using timestamp truncation
    from collections import defaultdict
    groups: defaultdict[datetime, list[MarketEvent]] = defaultdict(list)

    for bar in bar_buffer:
        key = _truncate_timestamp(bar.timestamp, target_tf)
        groups[key].append(bar)

    htf_bars = []
    for ts in sorted(groups):
        bars = groups[ts]
        htf_bar = MarketEvent(
            symbol=bars[0].symbol,
            timestamp=ts,
            open=bars[0].open,
            high=max(b.high for b in bars),
            low=min(b.low for b in bars),
            close=bars[-1].close,
            volume=sum(b.volume for b in bars),
            timeframe=target_tf,
        )
        htf_bars.append(htf_bar)

    return htf_bars
```

---

### 4.7 Different Trading Hours per Market

#### The Problem

When running `merge_bars()` across symbols with different trading hours, a
bar from "EURUSD" at 2026-02-22 03:00 UTC might arrive before any bar from
"AAPL" for the same day (NYSE opens at 14:30 UTC). Signals should not be
generated for NYSE stocks outside 14:30–21:00 UTC.

#### Solution: Session Filter Per Symbol

```python
from dataclasses import dataclass
from datetime import time, timezone

@dataclass
class TradingSession:
    """Defines valid trading hours for a market."""
    market_name: str
    open_time: time   # UTC
    close_time: time  # UTC
    trading_days: set[int]  # 0=Mon, 4=Fri (isoweekday - 1)

SESSIONS: dict[str, TradingSession] = {
    "NYSE":  TradingSession("NYSE",  time(14, 30), time(21, 0),  {0,1,2,3,4}),
    "FOREX": TradingSession("FOREX", time(22, 0),  time(22, 0),  {0,1,2,3,4}),
    # Forex is always open except weekends — use open=close as sentinel for 24h
    "CME_FUT": TradingSession("CME_FUT", time(23, 0), time(22, 0), {0,1,2,3,4}),
    # CME Futures: 23:00 Sunday to 22:00 Friday UTC (~23h/day)
}

SYMBOL_SESSION_MAP: dict[str, str] = {
    "AAPL":   "NYSE",
    "SPY":    "NYSE",
    "NQ":     "CME_FUT",
    "ES":     "CME_FUT",
    "EURUSD": "FOREX",
    "GBPUSD": "FOREX",
}

def is_in_session(bar: MarketEvent) -> bool:
    """Return True if bar's timestamp is within its market's trading session."""
    session_key = SYMBOL_SESSION_MAP.get(bar.symbol)
    if session_key is None:
        return True  # Unknown symbol: allow

    session = SESSIONS[session_key]
    ts_utc = bar.timestamp.astimezone(timezone.utc)

    # Weekend check
    if ts_utc.weekday() not in session.trading_days:
        return False

    # 24h markets (open == close sentinel)
    if session.open_time == session.close_time:
        return True

    t = ts_utc.time().replace(tzinfo=None)

    # Overnight session (e.g., 23:00–22:00 next day)
    if session.open_time > session.close_time:
        return t >= session.open_time or t <= session.close_time

    return session.open_time <= t <= session.close_time
```

Apply as a gate in `MultiAssetBacktestEngine.run()`:

```python
if not is_in_session(bar):
    self._portfolio.update_equity(bar)  # Still track equity
    continue  # But skip signal generation
```

---

### 4.8 Pitfalls and Edge Cases

#### Pitfall: Timestamp Collisions in Heap Merge

If two symbols have bars at exactly the same timestamp, the heap comparison
falls through to the `sym` string comparison. This is deterministic but
arbitrary. The ORDER of processing same-timestamp bars matters because
Portfolio state changes after the first one. Always process all same-timestamp
bars as an atomic group:

```python
# Collect all bars at the same timestamp before processing
group: list[MarketEvent] = []
current_ts = None
for bar in merge_bars(handlers):
    if current_ts is None:
        current_ts = bar.timestamp
    if bar.timestamp != current_ts:
        process_bar_group(group)
        group = [bar]
        current_ts = bar.timestamp
    else:
        group.append(bar)
if group:
    process_bar_group(group)
```

#### Pitfall: Forward-Filled Bars Generating Fake Signals

When `DataHandler.align_multi_symbol()` forward-fills a missing bar (sets
`volume = 1`), any strategy operating on that bar is operating on stale,
synthetic data. The strategy has no way to know this without explicit flagging.

**Solution:** Extend `MarketEvent` with an optional `is_synthetic: bool = False`
field, or filter bars with `volume == 1` in the strategy's `calculate_signals()`.

#### Pitfall: Correlation Computed on Misaligned Returns

If symbol A has a Monday bar (NYSE) and symbol B has Monday + Sunday bars
(Forex), their return series are different lengths and cannot be aligned by
simple index. The correlation monitor's `update()` method handles this
correctly by using per-symbol deques — as long as both are updated from
the merged bar stream, they grow at the same wall-clock rate.

#### Pitfall: Portfolio.compute_equity() with Stale Prices

`Portfolio.compute_equity()` takes a `prices: dict[str, Decimal]` argument.
In the multi-asset engine, only the current bar's symbol has a fresh price.
Other symbols use the last known price from the previous update. Track a
`last_prices: dict[str, Decimal]` cache in `MultiAssetBacktestEngine`:

```python
# In run():
self._last_prices[bar.symbol] = bar.close
equity = self._portfolio.compute_equity(self._last_prices)
```

---

### 4.9 Libraries (Free/Open-Source Only)

All four research areas require only Python stdlib + already-approved
dependencies:

| Library       | Use Case                                    | Status           |
|---------------|---------------------------------------------|------------------|
| `decimal`     | All financial math (prices, risk, sizing)   | stdlib, required |
| `collections` | `deque` for rolling windows, regime history  | stdlib           |
| `heapq`       | Multi-symbol chronological merge            | stdlib           |
| `zoneinfo`    | Kill zone timezone handling (Python 3.9+)   | stdlib           |
| `math`        | `sqrt` for correlation (float is fine here) | stdlib           |
| `pandas`      | Data loading only (already in requirements) | approved         |
| `yfinance`    | Data fetch (already in requirements)        | approved         |

No new dependencies needed. All algorithms are implementable with
stdlib + existing requirements.

---

## 5. Implementation Priority Map for v3.0

### Phase A: Risk Management Foundation (prerequisite for everything)
1. `RiskManager` class (fixed fractional + drawdown scaling + heat + max concurrent)
2. Extend `BacktestEngine._calculate_order_quantity()` to use `RiskManager`
3. Extend `BacktestEngine` to track `peak_equity`
4. Add `stop_atr_mult` parameter to strategies to expose ATR-based stop distance

### Phase B: Regime Detection
5. `ATRRegimeClassifier`
6. `ADXClassifier` (pure Decimal, Wilder's smoothing)
7. `RegimeClassifier` (combined)
8. `RegimeGatedStrategy` wrapper

### Phase C: ICT / Liquidity Layer
9. `KillZone` definitions + `in_kill_zone()` filter (requires timezone-aware timestamps)
10. `PremiumDiscountZone` + `compute_premium_discount()` (uses existing SwingDetector output)
11. `LiquiditySweepDetector` (uses existing SwingDetector output)
12. Integrate all into `SMCStrategy` as optional parameters

### Phase D: Multi-Asset Foundation
13. `merge_bars()` heap-based chronological merger
14. `MultiAssetBacktestEngine`
15. `CorrelationMonitor`
16. `TradingSession` / `is_in_session()` filter
17. `AssetRiskProfile` per-symbol configuration

### Phase E: Kelly Criterion (last — requires sufficient trade history)
18. `KellyPositionSizer` (enable after 20+ trades in RiskManager warmup)

---

## 6. Test Strategy for v3.0 (TDD)

### RiskManager Tests
```python
def test_fixed_fractional_size():
    # equity=10000, risk=1%, stop_dist=50 → qty = 10000*0.01/50 = 2
    assert rm.compute_quantity(Decimal('10000'), Decimal('10000'), Decimal('50'), []) == Decimal('2')

def test_drawdown_scales_size():
    # At 50% of max drawdown → size should be ~50% of normal
    ...

def test_max_concurrent_blocks_entry():
    # Open 3 positions → can_trade() returns False
    ...

def test_daily_risk_limit():
    # After 3% equity at risk today → block further trades
    ...
```

### Regime Detection Tests
```python
def test_atr_high_regime():
    # Feed bars with ATR spike → should classify HIGH
    ...

def test_adx_ranging_market():
    # Feed sideways bars → ADX < 20 → RANGING
    ...

def test_regime_gated_strategy_blocks_ranging():
    # SMC signal in RANGING regime → gated strategy returns None
    ...
```

### ICT Tests
```python
def test_kill_zone_london_open():
    # 07:30 UTC bar → in London Open Kill Zone
    ts = datetime(2026, 2, 22, 7, 30, tzinfo=timezone.utc)
    assert in_kill_zone(ts, KILL_ZONES).name == "London Open"

def test_kill_zone_blocks_outside_session():
    # 03:00 UTC (London midnight) → no kill zone
    ts = datetime(2026, 2, 22, 3, 0, tzinfo=timezone.utc)
    assert in_kill_zone(ts, KILL_ZONES) is None

def test_liquidity_sweep_single_bar():
    # bar.low < swing_low AND bar.close > swing_low → bullish sweep
    ...

def test_premium_discount_equilibrium():
    # price = midpoint → equilibrium zone
    zone = compute_premium_discount(Decimal('100'), Decimal('80'))
    assert price_zone(Decimal('90'), zone) == "equilibrium"
```

### Multi-Asset Tests
```python
def test_merge_bars_chronological():
    # Two symbols interleaved → output strictly ascending by timestamp
    ...

def test_correlation_monitor_perfect_correlation():
    # Same returns for A and B → r = 1.0
    ...

def test_is_in_session_nyse_closed_weekend():
    # Saturday 15:00 UTC, NYSE symbol → False
    ...

def test_forward_filled_bar_skipped():
    # volume == 1 bar → no signal generated
    ...
```

---

## 7. Key Architecture Invariants (Must Not Violate)

1. **No lookahead bias** — every new detector (sweep, IDM, kill zone, regime)
   must compute from `bar_buffer[:-0]` (current and past bars only). The
   fractal delay in `SwingDetector` (confirmed N bars late) is intentional
   and must be respected by all consumers.

2. **decimal.Decimal for all financial math** — position sizes, risk amounts,
   stop distances, PnL attribution all use Decimal. Correlation and regime
   ratios may use float (they are dimensionless).

3. **Stateless events** — `MarketEvent`, `SignalEvent`, `OrderEvent`,
   `FillEvent` remain frozen dataclasses. New ICT/regime context is carried
   in strategy instance state (pattern already established by `SMCStrategy`).

4. **Engine routes, strategy decides** — `BacktestEngine` and
   `MultiAssetBacktestEngine` have no trading logic. Regime gates, kill zone
   filters, sweep confirmations all live in `calculate_signals()`.

5. **RiskManager is the single source of truth for position sizing** — no
   sizing logic anywhere else. The `Portfolio.calculate_position_size()` method
   is superseded by `RiskManager.compute_quantity()`.

6. **Optimization safety** — `RegimeClassifier`, `ADXClassifier`,
   `CorrelationMonitor`, and `RiskManager` are all instantiated fresh per
   sweep iteration (they live inside strategies or the engine, which are
   re-created by `create_engine()`). No shared state between sweep runs.

---

*Research complete. Total research scope: 4 areas, ~120 algorithms/patterns documented.*
*Next step: Run `/gsd:new-milestone` to convert this into formal v3.0 REQUIREMENTS.md and ROADMAP.md.*
